{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science | Spring 2018 -- Resit\n",
    "\n",
    "***\n",
    "\n",
    "**The aim of this exam is to study the dataset composed of bike trips on the *Capital Bikeshare* network, based in Washington DC.** \n",
    "\n",
    "You're expected to download this notebook and add code and markdown cells to answer questions you'll find hereafter. The first couple of cells aim at loading the dataset as a python usable object. They are already written for you and only need to be executed for your objects to be available within notebook. You'll be then guided through a number of questions **exclusively** related to the biking dataset. \n",
    "\n",
    "**Your notebook has to be submitted by email to the address *bashar.dudin@epita.fr* 2:00 after start of examination. Subject has to be `[IPDS][RESIT] Spring 2018`.**\n",
    "\n",
    "Remember that :\n",
    "\n",
    "- **Any late submissisons will not be graded!**, unless there is a serious valid reason for it.\n",
    "- **Code cells** that raise errors should **NOT** be handed out.\n",
    "- **Copying googled code or your friends code is unlikely to help much.** You have access to the `python`, `numpy`, `pandas`, `matplotlib` and `sklearn` documentations. You do also have access to the course's notebooks. These are already enuough. For the rest ; gray matter should be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Dataset and Extracting It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "dataset_url = 'https://s3.amazonaws.com/capitalbikeshare-data/2011-capitalbikeshare-tripdata.zip'\n",
    "downloaded_file = 'Data/bikeshare-dataset-2011.zip'\n",
    "with urllib.request.urlopen(dataset_url) as response:\n",
    "    with open(downloaded_file, \"wb\") as out_file:\n",
    "        dataset_ = response.read()\n",
    "        out_file.write(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(downloaded_file, 'r')\n",
    "zip_ref.extractall('Data/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/2011-capitalbikeshare-tripdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer questions as fully as you can. You are free to give partial answers if you wish for as long as these **treat the studied dataset**, **make sense within the context** and **don't raise errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Looking into the Data\n",
    "\n",
    "We're dropping unuseful data and cleaning up to prepare for relevant analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I.1 Drop the columns corresponding to station names and bicycle identifiers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I.2 What does `mystery` function do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function is aimed at transforming two of the dataset columns into reusable data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __mystery(D, hash_t):\n",
    "    da, t = D.split(\" \")\n",
    "    \n",
    "    h, m, s = [int(x) for x in t.split(\":\")]\n",
    "    t_res = s + 60*m + h*3600\n",
    "    _, m, d = [int(x) for x in da.split(\"-\")]\n",
    "\n",
    "    da_res = 0\n",
    "    for i in range(1, m):\n",
    "        da_res += hash_t[i]\n",
    "    da_res += d - 1\n",
    "    da_res *= 24*3600\n",
    "    \n",
    "    return da_res + t_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystery(df, d_feature):\n",
    "    \"\"\"This is a a baby mystery function.\"\"\"\n",
    "    D = df[d_feature].iloc[0]\n",
    "    da, _ = D.split(\" \")\n",
    "    y = int(da.split(\"-\")[0])\n",
    "    l = y % 4 == 0 or (y % 100 != 0 and y % 400 == 0)\n",
    "    hash_t = {i : (30 + (i % 7 % 2 == 1) + (i == 7)) for i in range(1, 13)}\n",
    "    hash_t[2] = 28 + l\n",
    "    df[d_feature] = pd.Series([__mystery(D, hash_t) for D in df[d_feature]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery(data, \"Start date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery(data, \"End date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I.3 Recode `Member type` into a feature having either value `0` for `Casual` and `1` for `Member`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Building Up a First Naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get into any ML work you need to have a number of dummy models to which you compare increasingly complex models. Here is where we're doing such work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Our main **aim** is to predict `Member type` out of other relevant features in the dataset. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first work under the assumption that our only inputs are `Duration`, `Start date` and `End date`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.1 Decompose dataset into input and target, where input only contains `Duration`, `Start date` and `End date`. Expected types are `numpy` `arrays`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.2 Split datasets into training and testing sets having a test set proportion of `0.2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Model\n",
    "\n",
    "We're building a first constant model only answering `Member` independantly of input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.3 Compute accuracy score of a constant model only answering `Member` for any single entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.4 Following conventions for writing down a `Class` inheriting from the `BaseEstimator` class, define a model that always outputs `Member`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By a random model we mean here a model that would randomly answer either `Member` or `Casual` independantly of input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.6 Following steps in previous section write down and evaluate a random model that uniformly answers either `Member` or `Casual` independantly of entry. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.7 Using `binomial` from the `random` module within `numpy` build up a model that gets a closer to the constant model you've built at previous section (do not expect much).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. First ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try out two models on the dataset as it is now ; only containing `Duration`, `Start date` and `End date`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is one of the simplest models that is sensible to input ; with the difference with what we've been playing with uptill now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.1 Train and evaluate a `logistic regression`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about `random forest` model form the `sklearn` documentation page. It is a classifier that has often better results than a `logistic regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.2 Train and evaluate a `random forest`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.3 In both previous studies are all input features useful? You're expected to support your claim, either showing code or acceptable arguement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study any ML model, it is expected to be able to analyse learning curves of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.4 Plot learning curves of both previous models, up to the first `1000` entries of dataset (takes time).**\n",
    "\n",
    "You're allowed to reduce number of entries for performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III.5 Comment on results of both previous learning curves.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Taking Stations into Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we're not taking into account in our present models are the `Start Station` and `End Station`. It is enough to look into the numbers of these stations. The point is that these numbers do point to a categorical variable and are not a numerical one (the numbers do not have a mathematical significance here, you cannot compare them, add them etc.)\n",
    "\n",
    "In order to be able to feed such inputs to our model a standard strategy is to add extra dimensions to the dataset. Each new feature corresponds to a station name. A trip goes out from station A means that the only station feature at 1 is the one corresponding to station A the other being 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**IV.1 Extract from `data` the `numpy` `arrays` corresponding to `Start date`, `End date`, `Start station number` and `End station number`.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.2 Use your own words to explain what does `OneHotEncoder` estimator do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.3 Use a `OneHotEncoder` estimator to transform input dataset in order reincode your categorical variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.4 What is the shape of the transformed data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.6 Train a `logistic regression` on transformed data. Are results better than previously? You're invited to comment and support your claims.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.5 Train and evaluate an `SVC` on transformed data, using linear kernel.** \n",
    "\n",
    "You're allowed to first shrink your dataset, limiting observation to a `10000` or less if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.6 Choose a different `kernel` hyperparameter and train an new `SVC` classifier in order to get better results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Ideas to Get Better Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're free to try out what you feel would work. I'll comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are conclusions of your study? You're expected to support your claims either by looking into previously obtained results. Drawing partial learning curves. Making your own analysis and plots of errors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `Grid Search`? Support your study through use cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
